{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MENU\n",
      "1.LS for Logistic Regression\n",
      "2.DT for Decision Tree\n",
      "3.KN for K-Nearest Neighbor\n",
      "4.NB for Naive Bayes\n",
      "5.exit for Exit\n",
      "Enter Choice:quit\n",
      "Wrong Input!!!\n",
      "MENU\n",
      "1.LS for Logistic Regression\n",
      "2.DT for Decision Tree\n",
      "3.KN for K-Nearest Neighbor\n",
      "4.NB for Naive Bayes\n",
      "5.exit for Exit\n",
      "Enter Choice:exit\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Prabal\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3304: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "#Reading the csv file.\n",
    "df=pd.read_csv(\"forestfires.csv\")\n",
    "\n",
    "#************************************************************************\n",
    "\n",
    "#Converting column \"area\" into binary and adding to df.\n",
    "list1=[0 for i in range(517)]\n",
    "count=0\n",
    "for index,rows in df.iterrows():\n",
    "    if rows[\"area\"]==0:\n",
    "        list1[index]=0\n",
    "    else:\n",
    "        list1[index]=1\n",
    "area_b=pd.DataFrame(list1)\n",
    "df=pd.concat([df,area_b],axis=1)\n",
    "df.rename({0:\"area_b\"},axis=1,inplace=True)\n",
    "df.drop([\"area\"],axis=1,inplace=True)\n",
    "\n",
    "#Converting columns \"day\" and \"month\" into binary and adding to df.\n",
    "le_Gender=LabelEncoder() #Creating reference of LabelEncoder.\n",
    "df[\"month_b\"]=le_Gender.fit_transform(df[\"month\"])\n",
    "df[\"day_b\"]=le_Gender.fit_transform(df[\"day\"])\n",
    "df.drop([\"month\",\"day\"],axis=1,inplace=True)\n",
    "\n",
    "#***************************************************************************\n",
    "\n",
    "#Removing outleirs of column \"Y\".\n",
    "for index,row in df.iterrows():\n",
    "    if (row[\"Y\"]>6) or (row[\"Y\"]<3) :\n",
    "        df=df.drop(index)\n",
    "        \n",
    "#Removing outleirs of column \"FFMC\".\n",
    "for index,row in df.iterrows():\n",
    "    if (row[\"FFMC\"]>96) or (row[\"FFMC\"]<88) :\n",
    "        df=df.drop(index)\n",
    "        \n",
    "#Removing outleirs of column \"DMC\".\n",
    "for index,row in df.iterrows():\n",
    "    if (row[\"DMC\"]>215) or (row[\"DMC\"]<5) :\n",
    "        df=df.drop(index)\n",
    "        \n",
    "#Removing outleirs of column \"DC\".\n",
    "for index,row in df.iterrows():\n",
    "    if (row[\"DC\"]>800) or (row[\"DC\"]<450) :\n",
    "        df=df.drop(index)\n",
    "        \n",
    "#Removing outleirs of column \"ISI\".\n",
    "for index,row in df.iterrows():\n",
    "    if (row[\"ISI\"]>15.5) or (row[\"ISI\"]<0) :\n",
    "        df=df.drop(index)\n",
    "        \n",
    "#Removing outleirs of column \"temp\".\n",
    "for index,row in df.iterrows():\n",
    "    if (row[\"temp\"]>30) or (row[\"temp\"]<12) :\n",
    "        df=df.drop(index)\n",
    "        \n",
    "#After seeing so many outliers in column \"rain\", dropping \"rain\".\n",
    "df.drop([\"rain\"],axis=1,inplace=True)\n",
    "\n",
    "#Removal of Outliers completed.\n",
    "\n",
    "#***************************************************************************\n",
    "\n",
    "#Normailzation of the features.\n",
    "for column in [\"X\",\"Y\",\"FFMC\",\"DMC\",\"DC\",\"ISI\",\"temp\",\"RH\",\"wind\",\"month_b\",\"day_b\"]:\n",
    "    min_=float(df[column].min())\n",
    "    max_=float(df[column].max())\n",
    "    for index,row in df.iterrows():\n",
    "        df[column]=df[column].replace(row[column],(row[column]-min_)/(max_-min_))\n",
    "        \n",
    "#After normalization, column \"month_b\" has more than 90% of zeroes.\n",
    "#Hence dropping column \"month_b\".\n",
    "df.drop([\"month_b\"],axis=1,inplace=True)\n",
    "\n",
    "#***************************************************************************\n",
    "\n",
    "#Rearranging \"area_b\" to the last of the DataFrame.\n",
    "tempdf=pd.DataFrame(df[\"area_b\"])\n",
    "df.drop([\"area_b\"],axis=1,inplace=True)\n",
    "df[\"area_b\"]=tempdf\n",
    "\n",
    "#Renaming column \"X\" and \"Y\"\n",
    "df.rename({'X':'loc_x','Y':'loc_y'},axis=1,inplace=True)\n",
    "\n",
    "#****************************************************************************\n",
    "\n",
    "x=df.drop([\"area_b\"],axis=1) #Independent Variables\n",
    "y=df[\"area_b\"] #Dependent Variable\n",
    "\n",
    "#Importing common modules for all models.\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "while(True):\n",
    "    print(\"MENU\\n1.LS for Logistic Regression\\n2.DT for Decision Tree\\n3.KN for K-Nearest Neighbor\\n4.NB for Naive Bayes\\n5.exit for Exit\")\n",
    "    var=input(\"Enter Choice:\")\n",
    "    if var==\"LS\":\n",
    "        from sklearn.linear_model import LogisticRegression\n",
    "        #k_=int(input(\"Enter the number of top features to be added: \"))\n",
    "        #k_=4\n",
    "        for k_ in [3,4]:\n",
    "            #Feature selection in \"x\" on the basis of \"k_\"\n",
    "            bestfeatures=SelectKBest(score_func=chi2,k=k_)\n",
    "            fit=bestfeatures.fit(x,y)\n",
    "            dfscores=pd.DataFrame(fit.scores_)\n",
    "            dfscor=dfscores[0].sort_values(ascending=False)\n",
    "            features=dfscor.index.values\n",
    "            X=x.iloc[ : ,features[0:k_]]\n",
    "        \n",
    "            #Spliting \"X\" into test and train set.\n",
    "            x_train,x_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=100)\n",
    "            \n",
    "            #Calling LogisticRegression constructor.\n",
    "            logmodel=LogisticRegression()\n",
    "            #Training the model.\n",
    "            logmodel.fit(x_train,y_train)\n",
    "            #Predicting the model.\n",
    "            y_pred=logmodel.predict(x_test)\n",
    "            \n",
    "            print(\"For Top \",k_,\" Features, the analysis of Logistic Regression is as follow:\")\n",
    "            #Calculating Accuracy Score.\n",
    "            acc=accuracy_score(y_test,y_pred)\n",
    "            print(\"Accuracy Score is: \",acc)\n",
    "            \n",
    "            #Printing Confusion Matrix\n",
    "            print(\"Confusion Matrix is:\\n\",confusion_matrix(y_test,y_pred))\n",
    "            #Printing Classification Report\n",
    "            print(\"Classification Report is:\\n\",classification_report(y_test,y_pred))\n",
    "            print(\"*******************************************************************************\")\n",
    "        print(\"Hence the best fit model using Logistic Regression is achieved by using Top 4  features with Accuracy Score \",acc)\n",
    "        \n",
    "    elif var==\"DT\":\n",
    "        from sklearn.tree import DecisionTreeClassifier\n",
    "        #k_=int(input(\"Enter the number of top features to be added: \"))\n",
    "        #k_=4\n",
    "        for k_ in [3,4]:\n",
    "            #Feature selection in \"x\" on the basis of \"k_\"\n",
    "            bestfeatures=SelectKBest(score_func=chi2,k=k_)\n",
    "            fit=bestfeatures.fit(x,y)\n",
    "            dfscores=pd.DataFrame(fit.scores_)\n",
    "            dfscor=dfscores[0].sort_values(ascending=False)\n",
    "            features=dfscor.index.values\n",
    "            X=x.iloc[ : ,features[0:k_]]\n",
    "        \n",
    "            #Spliting \"X\" into test and train set.\n",
    "            x_train,x_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=100)\n",
    "            \n",
    "            #Calling DecisionTreeClassifier constructor.\n",
    "            classifier_entropy=DecisionTreeClassifier(criterion=\"entropy\",random_state=100,max_depth=5,min_samples_leaf=5)\n",
    "\n",
    "            #Training the model.\n",
    "            classifier_entropy.fit(x_train,y_train)\n",
    "            #Predicting the model.\n",
    "            y_pred=classifier_entropy.predict(x_test)\n",
    "            \n",
    "            print(\"For Top \",k_,\" Features, the analysis of Decision Tree is as follow:\")\n",
    "            #Calculating Accuracy Score.\n",
    "            acc=accuracy_score(y_test,y_pred)\n",
    "            print(\"Accuracy Score is: \",acc)\n",
    "            \n",
    "            #Printing Confusion Matrix\n",
    "            print(\"Confusion Matrix is:\\n\",confusion_matrix(y_test,y_pred))\n",
    "            #Printing Classification Report\n",
    "            print(\"Classification Report is:\\n\",classification_report(y_test,y_pred))\n",
    "            print(\"*******************************************************************************\")\n",
    "        print(\"Hence the best fit model using Decision Tree is achieved by using Top 4 features with Accuracy Score \",acc)\n",
    "                \n",
    "    elif var==\"KN\":\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        #k_=int(input(\"Enter the number of top features to be added: \"))\n",
    "        #k_=4\n",
    "        for k_ in [3,4]:\n",
    "            #Feature selection in \"x\" on the basis of \"k_\"\n",
    "            bestfeatures=SelectKBest(score_func=chi2,k=k_)\n",
    "            fit=bestfeatures.fit(x,y)\n",
    "            dfscores=pd.DataFrame(fit.scores_)\n",
    "            dfscor=dfscores[0].sort_values(ascending=False)\n",
    "            features=dfscor.index.values\n",
    "            X=x.iloc[ : ,features[0:k_]]\n",
    "        \n",
    "            #Spliting \"X\" into test and train set.\n",
    "            x_train,x_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=100)\n",
    "            \n",
    "            #Calling KNeighborsClassifier constructor.\n",
    "            import math as m\n",
    "            b=int(m.sqrt(len(x))) #Taking square root of total number of instances.\n",
    "            classifier=KNeighborsClassifier(n_neighbors=b,p=2,metric='euclidean')\n",
    "            \n",
    "            #Training the model.\n",
    "            classifier.fit(x_train,y_train)\n",
    "            #Predicting the model.\n",
    "            y_pred=classifier.predict(x_test)\n",
    "            \n",
    "            print(\"For Top \",k_,\" Features, the analysis of K-Nearest Neighbor is as follow:\")\n",
    "            #Calculating Accuracy Score.\n",
    "            acc=accuracy_score(y_test,y_pred)\n",
    "            print(\"Accuracy Score is: \",acc)\n",
    "            \n",
    "            #Printing Confusion Matrix\n",
    "            print(\"Confusion Matrix is:\\n\",confusion_matrix(y_test,y_pred))\n",
    "            #Printing Classification Report\n",
    "            print(\"Classification Report is:\\n\",classification_report(y_test,y_pred))\n",
    "            print(\"*******************************************************************************\")\n",
    "        print(\"Hence the best fit model using K-Nearest Neighbor is achieved by using Top 4 features with Accuracy Score \",acc)\n",
    "    elif var==\"NB\":\n",
    "        from sklearn.naive_bayes import GaussianNB\n",
    "        #k_=int(input(\"Enter the number of top features to be added: \"))\n",
    "        #k_=4\n",
    "        for k_ in [3,4]:\n",
    "            #Feature selection in \"x\" on the basis of \"k_\"\n",
    "            bestfeatures=SelectKBest(score_func=chi2,k=k_)\n",
    "            fit=bestfeatures.fit(x,y)\n",
    "            dfscores=pd.DataFrame(fit.scores_)\n",
    "            dfscor=dfscores[0].sort_values(ascending=False)\n",
    "            features=dfscor.index.values\n",
    "            X=x.iloc[ : ,features[0:k_]]\n",
    "        \n",
    "            #Spliting \"X\" into test and train set.\n",
    "            x_train,x_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=100)\n",
    "            \n",
    "            #Calling DecisionTreeClassifier constructor.\n",
    "            model=GaussianNB()\n",
    "            \n",
    "            #Training the model.\n",
    "            model.fit(x_train,y_train)\n",
    "            #Predicting the model.\n",
    "            y_pred=model.predict(x_test)\n",
    "            \n",
    "            print(\"For Top \",k_,\" Features, the analysis of Naive Bayes is as follow:\")\n",
    "            #Calculating Accuracy Score.\n",
    "            acc=accuracy_score(y_test,y_pred)\n",
    "            print(\"Accuracy Score is: \",acc)\n",
    "            if k_==3:\n",
    "                final_acc=acc\n",
    "            #Printing Confusion Matrix\n",
    "            print(\"Confusion Matrix is:\\n\",confusion_matrix(y_test,y_pred))\n",
    "            #Printing Classification Report\n",
    "            print(\"Classification Report is:\\n\",classification_report(y_test,y_pred))\n",
    "            print(\"*******************************************************************************\")\n",
    "        print(\"Hence the best fit model using Naive Bayes is achieved by using Top 3 features with Accuracy Score \",final_acc)\n",
    "    elif var==\"exit\":\n",
    "        sys.exit(0)\n",
    "    else:\n",
    "        print(\"Wrong Input!!!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-b66ca40dc9ba>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscatterplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
